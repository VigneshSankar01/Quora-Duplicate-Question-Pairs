{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a875cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "\n",
    "import string\n",
    "exclude = string.punctuation\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad282f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b134cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf =  df.sample(50000,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['question1'] = newdf['question1'].astype('string')\n",
    "newdf['question2'] = newdf['question2'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4927838",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e6726",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e330f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['question1'] = newdf['question1'].str.lower()\n",
    "newdf['question2'] = newdf['question2'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "df['question1'] = newdf['question1'].apply(rem_html_tags)\n",
    "df['question2'] = newdf['question2'].apply(rem_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd6a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "newdf['question1'] = newdf['question1'].apply(rem_url)\n",
    "newdf['question2'] = newdf['question2'].apply(rem_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puncs(text):\n",
    "    return text.translate(str.maketrans('','',exclude))\n",
    "\n",
    "newdf['question1'] = newdf['question1'].apply(remove_puncs)\n",
    "newdf['question2'] = newdf['question2'].apply(remove_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2acb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    text = text.replace('%',' percent ')\n",
    "    text = text.replace('$',' dollar ')\n",
    "    text = text.replace('₹',' rupee ')\n",
    "    text = text.replace('€',' euro ')\n",
    "    text = text.replace('@',' at ')\n",
    "    \n",
    "    text = text.replace('[math]','')\n",
    "    \n",
    "    #Some common contractions \n",
    "    # found on stack overflow\n",
    "    # https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "    \n",
    "    contractions = { \n",
    "    \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is / how does\",\n",
    "    \"I'd\": \"I had / I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I shall / I will\",\n",
    "    \"I'll've\": \"I shall have / I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "   \n",
    "    d_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "        d_text.append(word)\n",
    "        \n",
    "    text = ' '.join(d_text)\n",
    "    text = text.replace(\"'ve\", \"have\")\n",
    "    text = text.replace(\"'re\", \"are\")\n",
    "    text = text.replace(\"n't\", \"not\")\n",
    "    text = text.replace(\"'ll\", \"will\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['question1'] = newdf['question1'].apply(preprocess)\n",
    "newdf['question2'] = newdf['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenization(text):\n",
    "    return nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['question1'] = newdf['question1'].apply(tokenization)\n",
    "newdf['question2'] = newdf['question2'].apply(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce31536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_actual_question(s):\n",
    "    return \"\".join(s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['question1'] = newdf['question1'].apply(return_actual_question)\n",
    "newdf['question2'] = newdf['question2'].apply(return_actual_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ca7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858f35a",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2533e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['q1len'] = newdf['question1'].apply(lambda x:len(x))\n",
    "newdf['q2len'] = newdf['question2'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countwords(text):\n",
    "    count=0\n",
    "    for word in text.split(' '):\n",
    "        count+=1\n",
    "    return count\n",
    "\n",
    "newdf['q1words'] = newdf['question1'].apply(countwords)\n",
    "newdf['q2words'] = newdf['question2'].apply(countwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words(row):\n",
    "    # Splitting each question into words and converting to lowercase\n",
    "    words1 = set(map(lambda x: x.lower(), row['question1'].split()))\n",
    "    words2 = set(map(lambda x: x.lower(), row['question2'].split()))\n",
    "    # Finding the common words\n",
    "    common_words_set = words1.intersection(words2)\n",
    "    return len(common_words_set)\n",
    "newdf['common_words'] = newdf.apply(common_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_words(row):\n",
    "    w1 = set(map(lambda x: x.lower(), row['question1'].split()))\n",
    "    w2 = set(map(lambda x: x.lower(), row['question2'].split()))\n",
    "    return len(w1) + len(w2)\n",
    "newdf['total_words'] = newdf.apply(total_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['wordshare'] = round(newdf['common_words'] / newdf['total_words'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246acae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(newdf, x='common_words', hue='is_duplicate', kde=True, common_norm=False)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(newdf, x='total_words', hue='is_duplicate', kde=True, common_norm=False)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82635b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(newdf, x='wordshare', hue='is_duplicate', kde=True, common_norm=False)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f4fef",
   "metadata": {},
   "source": [
    "### Advanced Features\n",
    "\n",
    "- Length Based Features:\n",
    "1. Mean Length - Avg of the 2 question lengths\n",
    "2. abs_len_diff - Abs difference b/w the lengths of the 2 qns\n",
    "3. Longest_substr_ratio - Ratio of len of longest sub string to the question with a smaller length\n",
    "\n",
    "- Token Features\n",
    "1. cwc_min - Ratio of Common words to the smaller qn\n",
    "2. cwc_max - Ratio of Common words to the lengthier qn\n",
    "3. csc_min - ratio of common stop words to the smaller stop word count among 2 qns\n",
    "4. csc_man - ratio of common stop words to the larger stop word count among 2 qns\n",
    "5. ctc_min - Ratio of common tokens to the smalller token count among 2 qns\n",
    "6. ctc_max - Ratio of common tokens to the larger token count among 2 qns\n",
    "7. last_word_eq - 1 if the last word of the qns are the same\n",
    "8. first_word_eq - 0 if the last word of the qns are the same\n",
    "\n",
    "- Fuzzy Features\n",
    "1. fuzz_ratio - Fuzz ratio score from fuzzwuzzy\n",
    "2. fuzz_partial_ratio - from fuzzwuzzy\n",
    "3. token_sort_ratio - from fuzzwuzzy\n",
    "4. token_set_ratio - from fuzzwuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f310f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenfeatures(row):\n",
    "    \n",
    "    tkft = [0]*8\n",
    "    \n",
    "    \n",
    "    min_len = min(row['q1len'], row['q2len'])\n",
    "    max_len = max(row['q1len'], row['q2len'])\n",
    "    \n",
    "    q1tokens = row['question1'].split()\n",
    "    q2tokens = row['question2'].split()\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    common_tokens = set([word for word in q1tokens if word in q2tokens])\n",
    "    \n",
    "    q1_stopwords = set([word for word in q1tokens if word in stop_words])\n",
    "    q2_stopwords = set([word for word in q2tokens if word in stop_words])\n",
    "    \n",
    "    \n",
    "    \n",
    "    common_stopwords = q1_stopwords.intersection(q2_stopwords)\n",
    "    \n",
    "    nr1 = min(len(q1_stopwords),len(q2_stopwords))\n",
    "    nr2 = max(len(q1_stopwords),len(q2_stopwords))\n",
    "    nr3 = min(len(q1tokens),len(q2tokens))\n",
    "    nr4 = max(len(q1tokens),len(q2tokens))\n",
    "    \n",
    "    \n",
    "    tkft[0] = round(row['common_words'] / min_len, 2) if min_len != 0 else 0.0\n",
    "    tkft[1] = round(row['common_words'] / max_len, 2) if max_len != 0 else 0.0\n",
    "    tkft[2] = round(len(common_stopwords)/nr1,2) if nr1 != 0 else 0.0\n",
    "    tkft[3] = round(len(common_stopwords)/nr2,2) if nr2 != 0 else 0.0\n",
    "    tkft[4] = round(len(common_tokens)/ nr3,2) if nr3 != 0 else 0.0\n",
    "    tkft[5] = round(len(common_tokens)/ nr4,2) if nr4 != 0 else 0.0\n",
    "    tkft[6] = int(q1tokens[-1] == q2tokens[-1])\n",
    "    tkft[7] = int(q1tokens[0] == q2tokens[0])\n",
    "    \n",
    "    return tkft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49040c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenfeatures = newdf.apply(tokenfeatures,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['cwc_min'] = list(map(lambda x: x[0], tokenfeatures))\n",
    "newdf['cwc_max'] = list(map(lambda x: x[1], tokenfeatures))\n",
    "newdf['csc_min'] = list(map(lambda x: x[2], tokenfeatures))\n",
    "newdf['csc_max'] = list(map(lambda x: x[3], tokenfeatures))\n",
    "newdf['ctc_min'] = list(map(lambda x: x[4], tokenfeatures))\n",
    "newdf['ctc_max'] = list(map(lambda x: x[5], tokenfeatures))\n",
    "newdf['last_word_eq'] = list(map(lambda x: x[6], tokenfeatures))\n",
    "newdf['first_word_eq'] = list(map(lambda x: x[7], tokenfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b435b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "\n",
    "def length_features(row):\n",
    "    \n",
    "    lenft = [0]*3\n",
    "    \n",
    "    q1tokens = row['question1'].split()\n",
    "    q2tokens = row['question2'].split()\n",
    "    \n",
    "    lenft[0] = lenft[0] = round((len(q1tokens) + len(q2tokens)) / 2, 2)\n",
    "\n",
    "\n",
    "    lenft[1] = abs((len(q1tokens) - len(q2tokens)))\n",
    "    \n",
    "    nr5 = min(len(q1tokens),len(q2tokens))\n",
    "    \n",
    "    subs = list(distance.lcsubstrings(row['question1'],row['question2']))\n",
    "    \n",
    "    lenft[2] = round(len(subs) / nr5, 2) if nr5 != 0 else 0.0\n",
    "                     \n",
    "    return lenft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthfeatures = newdf.apply(length_features,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['mean_len'] = list(map(lambda x: x[0], lengthfeatures))\n",
    "newdf['abs_len_diff'] = list(map(lambda x: x[1], lengthfeatures))\n",
    "newdf['longest_substr_ratio'] = list(map(lambda x: x[2], lengthfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fetch_fuzzyfeatures(row):\n",
    "    \n",
    "    fuzzfeat = [0]*4\n",
    "    \n",
    "    fuzzfeat[0] = fuzz.QRatio(row['question1'],row['question2'])\n",
    "    \n",
    "    fuzzfeat[1] = fuzz.partial_ratio(row['question1'],row['question2'])\n",
    "    \n",
    "    fuzzfeat[2] = fuzz.token_sort_ratio(row['question1'],row['question2'])\n",
    "    \n",
    "    fuzzfeat[3] = fuzz.token_set_ratio(row['question1'],row['question2'])\n",
    "    \n",
    "    return fuzzfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzyfeatures = newdf.apply(fetch_fuzzyfeatures,axis=1)\n",
    "\n",
    "newdf['fuzz_ratio'] = list(map(lambda x: x[0], fuzzyfeatures))\n",
    "newdf['full_partial_ratio'] = list(map(lambda x: x[1], fuzzyfeatures))\n",
    "newdf['full_sort_ratio'] = list(map(lambda x: x[2], fuzzyfeatures))\n",
    "newdf['full_set_ratio'] = list(map(lambda x: x[3], fuzzyfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02937fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834eb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quesdf = newdf[['question1','question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf002490",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = newdf.drop(columns=['id','qid1','qid2','question1','question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "questions = list(quesdf['question1'].fillna('')) + list(quesdf['question2'].fillna(''))\n",
    "\n",
    "# Call the CountVectorizer object\n",
    "cv = CountVectorizer(max_features=3000)\n",
    "q1, q2 = np.vsplit(cv.fit_transform(questions).toarray(), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f78a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf1 = pd.DataFrame(q1, index = newdf.index)\n",
    "tdf2 = pd.DataFrame(q2, index = newdf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowdf = pd.concat([tdf1,tdf2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "findf = pd.concat([final_df,bowdf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = findf.iloc[:,1:]\n",
    "y = findf.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44645604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values,y.values,test_size=0.2,random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xbg = XGBClassifier()\n",
    "xbg.fit(X_train,y_train)\n",
    "y_pred1 = xbg.predict(X_test)\n",
    "accuracy_score(y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea023e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')  # You can choose different kernels like 'linear', 'poly', 'rbf', etc.\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred2 = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred2)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc638b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
